This project is a part in the completion of Computer Engineering Course designed by the Institute of Engineering(IOE), Tribhuvan University.  

This project demonstrates a real-time image animation system using the First Order Motion Model (FOMM) and Generative Adversarial Networks (GANs). It enables the dynamic animation of a static image by transferring motion from a driving video, effectively bringing portraits to life with real-time facial motion.

**Overview**

Objective: Animate still images using motion cues extracted from a source video in real time.

Core Technologies:
1) FOMM (First Order Motion Model) for unsupervised motion transfer.
2) OpenCV for video capture, frame processing, and real-time integration.
3) PyTorch for deep learning model implementation and inference.
4) GANs to synthesize high-quality animated frames.

**Features**

1) Real-time motion transfer from video input to still images.
2) Lightweight and optimized for performance on modern hardware.
3) Modular code structure for easy experimentation and extension.
4) Supports webcam and video file as driving input.
5) Smooth and lifelike animation of facial features with minimal latency.

**Requirements**

1) Python 3.8+
2) PyTorch
3) OpenCV
4) NumPy
5) imageio
6) Cuda/MTS

(this repository is consists the project suitable for systems with or without cuda compatibility. So it allows the project demonstration with an uploaded demo video rather than in real time.)

check the Major Project repo for the complete cuda-compatible project.


